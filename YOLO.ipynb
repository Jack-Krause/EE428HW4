{"cells":[{"cell_type":"markdown","metadata":{"id":"3Uvm-tUSeEeN"},"source":["## Header\n","Please put your name and student ID number here as usual."]},{"cell_type":"markdown","metadata":{"id":"EjmpZca5eEeN"},"source":["# YOLO V3 Object Detection\n","\n","## 1. Introduction\n","\n","\n","As you learned in class, YOLO is a state-of-the-art, real-time object detection algorithm. In this notebook, we will apply the YOLO V3 algorithm to detect objects in images. We have provided a series of images that you can test the YOLO algorithm on and explain why it gave you the results that it did.\n","\n","The questions in the last section are based on the paper \"YOLOv3: An Incremental Improvement,\"\n","Joseph Redmon, Ali Farhadi https://arxiv.org/abs/1804.02767.\n","\n","\n","There are named images that you can use to test the system. For the problems in the last sections, you will use the images with names that start from VOC to test system performance.\n","\n","These images are located in the`./images/`folder. We encourage you to test the YOLO algorithm on your own images as well."]},{"cell_type":"markdown","metadata":{"id":"al1E9TmweEeP"},"source":["## 1.1 Acknowledgements\n","This code was created by Garima Nishad.\n","\n","For detailed explanation you can refer to her blog here: https://towardsdatascience.com/you-only-look-once-yolo-implementing-yolo-in-less-than-30-lines-of-python-code-97fb9835bfd2?source=friends_link&sk=5c7234f716e38c8a7b6625ef20fa7811\n","\n","This was inspired from CVND Udacity course\n","\n","LinkedIn: https://www.linkedin.com/in/garima-nishad-9b8385134/\n","\n"]},{"cell_type":"markdown","metadata":{"id":"l69ELrsaeEeP"},"source":["# Importing Resources\n","\n","We will start by loading the required packages into Python. We will be using *OpenCV* to load our images, *matplotlib* to plot them, a`utils` module that contains some helper functions, and a modified version of *Darknet*. YOLO uses *Darknet53*, an open source, deep neural network framework written by the creators of YOLO. The version of *Darknet* used in this notebook has been modified to work in PyTorch 0.4 and has been simplified because we won't be doing any training. Instead, we will be using a set of pre-trained weights that were trained on the Common Objects in Context (COCO) database. For more information on *Darknet*, please visit <a href=\"https://pjreddie.com/darknet/\">Darknet</a>."]},{"cell_type":"markdown","metadata":{"id":"kQndOAmiVTO3"},"source":["# Setup Code\n","Before getting started we need to run some boilerplate code to set up our environment. You'll need to rerun this setup code each time you start the notebook.\n"]},{"cell_type":"markdown","metadata":{"id":"bCtoiSyVVTO8"},"source":["### Google Colab Setup\n","We need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n","\n","Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":595,"status":"ok","timestamp":1762888702327,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"},"user_tz":360},"id":"tHG0slB6VTO8","outputId":"806f46ac-b483-4f13-9f17-685059edb5fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1762888702342,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"},"user_tz":360},"id":"KqMvJnNHVTPA","outputId":"6b429151-a735-467a-b6bd-b4239d4781cd"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab_EE428/Colab_S2024/A4_new\n","['YOLO.ipynb', 'images', '__pycache__', 'iti', 'cfg', '.git', 'LICENSE', 'README.md', 'darknet.py', 'utils.py', '.gitignore', 'YOLO.py']\n"]}],"source":["import os\n","\n","# TODO: Fill in the Google Drive path where you uploaded the assignment\n","# Example: If you create a 2024S folder and put all the files under A1 folder, then '2024S/A1'\n","# GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = '2024S/A1'\n","#/content/drive/MyDrive/Colab_EE428/Colab_S2024/A1\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'Colab_EE428/Colab_S2024/A4_new'\n","GOOGLE_DRIVE_PATH = os.path.join('/content/drive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","print(GOOGLE_DRIVE_PATH)\n","print(os.listdir(GOOGLE_DRIVE_PATH))"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"VGbUf6nTfWzV","executionInfo":{"status":"ok","timestamp":1762888702343,"user_tz":360,"elapsed":1,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"}}},"outputs":[],"source":["import sys\n","sys.path.append(GOOGLE_DRIVE_PATH)\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4013,"status":"ok","timestamp":1762888706356,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"},"user_tz":360},"id":"R-Y_FxTZeEeP","outputId":"5d87d66b-303b-44e3-ebb0-8c4418f4f9ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n","Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n"]}],"source":["pip install opencv-python"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"ZVVokrFjeEeQ","executionInfo":{"status":"ok","timestamp":1762888711154,"user_tz":360,"elapsed":4797,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"}}},"outputs":[],"source":["import os\n","import cv2\n","import matplotlib.pyplot as plt\n","\n","from utils import *\n","from darknet import Darknet"]},{"cell_type":"markdown","metadata":{"id":"CHU8O26ZeEeQ"},"source":["# Setting Up The Neural Network\n","\n","We will be using YOLOv3. We have already downloaded the `yolov3.cfg` file that contains the network architecture used by YOLOv3 and placed it in the `/cfg/` folder. Similarly, we have placed the `yolov3.weights` file that contains the pre-trained weights in the `/weights/` directory. Finally, the `/data/` directory, contains the `coco.names` file that has the list of the 80 object classes that the weights were trained to detect.\n","\n","In the code below, we start by specifying the location of the files that contain the neural network architecture, the pre-trained weights, and the object classes.  We then use *Darknet* to setup the neural network using the network architecture specified in the `cfg_file`. We then use the`.load_weights()` method to load our set of pre-trained weights into the model. Finally, we use the `load_class_names()` function, from the `utils` module, to load the 80 object classes."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":377},"executionInfo":{"elapsed":518,"status":"error","timestamp":1762888711674,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"},"user_tz":360},"id":"7ZP1pxBreEeR","outputId":"0fecf5f0-1a87-416e-d4d0-532eae441387"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/Colab_EE428/Colab_S2024/A4_new/weights/yolov3.weights'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3119885384.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load the pre-trained weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# # Set the location and name of the COCO object classes file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Colab_EE428/Colab_S2024/A4_new/darknet.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, weightfile)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweightfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweightfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab_EE428/Colab_S2024/A4_new/weights/yolov3.weights'"]}],"source":["# Set the location and name of the cfg file\n","cfg_file = os.path.join(GOOGLE_DRIVE_PATH, 'cfg/yolov3.cfg')\n","# Load the network architecture\n","m = Darknet(cfg_file)\n","\n","# # Set the location and name of the pre-trained weights file\n","weight_file = os.path.join(GOOGLE_DRIVE_PATH, 'weights/yolov3.weights')\n","\n","# Load the pre-trained weights\n","m.load_weights(weight_file)\n","\n","# # Set the location and name of the COCO object classes file\n","namesfile = os.path.join(GOOGLE_DRIVE_PATH, 'data/coco.names')\n","\n","print(cfg_file,weight_file,namesfile)\n","# # Load the COCO object classes\n","class_names = load_class_names(namesfile)"]},{"cell_type":"markdown","metadata":{"id":"8F_32Vq8eEeR"},"source":["# The Darknet Backbone for object detection\n","\n","Now that the neural network has been setup, we can see what it looks like. We can print the network using the `.print_network()` function."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10151,"status":"aborted","timestamp":1762888711673,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"},"user_tz":360},"id":"Qy5exmwReEeR","scrolled":false},"outputs":[],"source":["# Print the neural network used in YOLOv3\n","m.print_network()"]},{"cell_type":"markdown","metadata":{"id":"-B1kq1BHeEeS"},"source":["As we can see, the neural network used by YOLOv3 consists mainly of convolutional layers, with some shortcut connections and upsample layers. For a full description of this network please refer to the <a href=\"https://pjreddie.com/media/files/papers/YOLOv3.pdf\">YOLOv3 Paper</a>.\n","\n","# Loading and Resizing Our Images\n","\n","In the code below, we load our images using OpenCV's `cv2.imread()` function. Since, this function loads images as BGR we will convert our images to RGB so we can display them with the correct colors.\n","\n","As we can see in the previous cell, the input size of the first layer of the network is 416 x 416 x 3. Since images have different sizes, we have to resize our images to be compatible with the input size of the first layer in the network. In the code below, we resize our images using OpenCV's `cv2.resize()` function. We then plot the original and resized images."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10203,"status":"aborted","timestamp":1762888711727,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"},"user_tz":360},"id":"TmN3N6QFeEeS"},"outputs":[],"source":["# Set the default figure size\n","plt.rcParams['figure.figsize'] = [24.0, 14.0]\n","\n","# Load the image\n","img = cv2.imread(os.path.join(GOOGLE_DRIVE_PATH,'images/brockp.jpeg'))\n","\n","# Convert the image to RGB\n","original_image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","# We resize the image to the input width and height of the first layer of the network.\n","resized_image = cv2.resize(original_image, (m.width, m.height))\n","\n","# Display the images\n","plt.subplot(121)\n","plt.title('Original Image')\n","plt.imshow(original_image)\n","plt.subplot(122)\n","plt.title('Resized Image')\n","plt.imshow(resized_image)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"e-H0UUXjeEeT"},"source":["# Setting the Non-Maximal Suppression Threshold\n","\n","As you learned in the previous lessons, YOLO uses **Non-Maximal Suppression (NMS)** to only keep the best bounding box. The first step in NMS is to remove all the predicted bounding boxes that have a detection probability that is less than a given NMS threshold.  In the code below, we set this NMS threshold to `0.6`. This means that all predicted bounding boxes that have a detection probability less than 0.6 will be removed."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pqqGVVcveEeT","executionInfo":{"status":"aborted","timestamp":1762888711727,"user_tz":360,"elapsed":10203,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"}}},"outputs":[],"source":["# Set the NMS threshold\n","nms_thresh = 0.6"]},{"cell_type":"markdown","metadata":{"id":"ylwzsYxLeEeT"},"source":["# Setting the Intersection Over Union Threshold\n","\n","After removing all the predicted bounding boxes that have a low detection probability, the second step in NMS, is to select the bounding boxes with the highest detection probability and eliminate all the bounding boxes whose **Intersection Over Union (IOU)** value is higher than a given IOU threshold. In the code below, we set this IOU threshold to `0.4`. This means that all predicted bounding boxes that have an IOU value greater than 0.4 with respect to the best bounding boxes will be removed.\n","\n","In the `utils` module you will find the `nms` function, that performs the second step of Non-Maximal Suppression, and the `boxes_iou` function that calculates the Intersection over Union of two given bounding boxes. You are encouraged to look at these functions to see how they work."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HqgVGApneEeT","executionInfo":{"status":"aborted","timestamp":1762888711728,"user_tz":360,"elapsed":10204,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"}}},"outputs":[],"source":["# Set the IOU threshold\n","iou_thresh = 0.4"]},{"cell_type":"markdown","metadata":{"id":"-Go8G43NeEeT"},"source":["# Object Detection\n","\n","Once the image has been loaded and resized, and you have chosen your parameters for `nms_thresh` and `iou_thresh`, we can use the YOLO algorithm to detect objects in the image. We detect the objects using the `detect_objects(m, resized_image, iou_thresh, nms_thresh)`function from the `utils` module. This function takes in the model `m` returned by *Darknet*, the resized image, and the NMS and IOU thresholds, and returns the bounding boxes of the objects found.\n","\n","Each bounding box contains 7 parameters: the coordinates *(x, y)* of the center of the bounding box, the width *w* and height *h* of the bounding box, the confidence detection level, the object class probability, and the object class id. The `detect_objects()` function also prints out the time it took for the YOLO algorithm to detect the objects in the image and the number of objects detected. Since we are running the algorithm on a CPU it takes about 2 seconds to detect the objects in an image, however, if we were to use a GPU it would run much faster.\n","\n","Once we have the bounding boxes of the objects found by YOLO, we can print the class of the objects found and their corresponding object class probability. To do this we use the `print_objects()` function in the `utils` module.\n","\n","Finally, we use the `plot_boxes()` function to plot the bounding boxes and corresponding object class labels found by YOLO in our image. If you set the `plot_labels` flag to `False` you will display the bounding boxes with no labels. This makes it easier to view the bounding boxes if your `nms_thresh` is too low. The `plot_boxes()`function uses the same color to plot the bounding boxes of the same object class. However, if you want all bounding boxes to be the same color, you can use the `color` keyword to set the desired color. For example, if you want all the bounding boxes to be red you can use:\n","\n","`plot_boxes(original_image, boxes, class_names, plot_labels = True, color = (1,0,0))`\n","\n","You are encouraged to change the `iou_thresh` and `nms_thresh` parameters to see how they affect the YOLO detection algorithm. The default values of `iou_thresh = 0.4` and `nms_thresh = 0.6` work well to detect objects in different kinds of images. In the cell below, we have repeated some of the code used before in order to prevent you from scrolling up down when you want to change the `iou_thresh` and `nms_thresh`parameters or the image. Have Fun!"]},{"cell_type":"markdown","metadata":{"id":"NtlfTSfHeEeU"},"source":["### 2.1 What Network Sees\n","\n","The code in this section shows all the boxes found by the system when the thresholds are set to zero. This means that there is no filtering of the \"objectness\" probability. Note the number of boxes found and plotted and the number of nonsensical results. The confidence level given in the output is the confidence found by the object recognition system, NOT the \"objectness\" probability. The objectness probability is found in the fifth element of the bounding box vector shown below it."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10202,"status":"aborted","timestamp":1762888711728,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"},"user_tz":360},"id":"zLF8ZKIueEeU"},"outputs":[],"source":["# Set the default figure size\n","plt.rcParams['figure.figsize'] = [24.0, 14.0]\n","\n","# Load the image\n","\n","img = cv2.imread(os.path.join(GOOGLE_DRIVE_PATH,'images/brockp.jpeg'))\n","\n","# Convert the image to RGB\n","original_image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","# We resize the image to the input width and height of the first layer of the network.\n","resized_image = cv2.resize(original_image, (m.width, m.height))\n","\n","# Set the IOU threshold. Default value is 0.4\n","iou_thresh = 0.2\n","\n","# Set the NMS threshold. Default value is 0.6\n","nms_thresh = 0.4\n","\n","# Detect objects in the image\n","boxes_all = detect_objects(m, resized_image, iou_thresh, nms_thresh)\n","\n","# Print the objects found and the confidence level and box information\n","for i in range(len(boxes_all)):\n","        box = boxes_all[i]\n","\n","        if len(box) >= 7 and class_names:\n","            obj_conf = box[5]\n","            obj_coord = [box[1],box[2],box[3],box[4]]\n","            cls_conf = box[5]\n","            cls_id = box[6]\n","\n","            print('%i. %s %f ' % (i + 1, class_names[cls_id], cls_conf))\n","            print(box)\n","\n","\n","#Plot the image with bounding boxes and corresponding object class labels\n","plot_boxes(original_image, boxes_all, class_names, plot_labels = True)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1tpOVn0LIhoZ","executionInfo":{"status":"aborted","timestamp":1762888711729,"user_tz":360,"elapsed":10203,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"}}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"_y_MSkhteEeU"},"source":["### 2.2 Effects of NMS and IoU Processing\n","This is the code that you will use to answer the questions in Section 3. You will need to experiment with differnt images, either from VOC or your own images to investigate the effects of the different thresholds."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10203,"status":"aborted","timestamp":1762888711730,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"},"user_tz":360},"id":"Ffx5oe3JeEeU"},"outputs":[],"source":["# Set the default figure size\n","plt.rcParams['figure.figsize'] = [24.0, 14.0]\n","\n","# Load the image\n","img = cv2.imread(os.path.join(GOOGLE_DRIVE_PATH,'images/brockp.jpeg'))\n","\n","# Convert the image to RGB\n","original_image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","# We resize the image to the input width and height of the first layer of the network.\n","resized_image = cv2.resize(original_image, (m.width, m.height))\n","\n","# Set the IOU threshold. Default value is 0.4\n","iou_thresh = 0.4\n","\n","# Set the NMS threshold. Default value is 0.6\n","nms_thresh = 0.6\n","\n","\n","# Detect objects in the image\n","boxes = detect_objects(m, resized_image, iou_thresh, nms_thresh)\n","\n","# Print the objects found and the confidence level\n","print_objects(boxes, class_names)\n","\n","#Plot the image with bounding boxes and corresponding object class labels\n","plot_boxes(original_image, boxes, class_names, plot_labels = True)"]},{"cell_type":"markdown","metadata":{"id":"DIZ4xWBAeEeU"},"source":["## 3.0 Homework Assignment\n","\n","Before answering any questions, please be sure that you have read at least the YOLO V3 paper. The list of foundational YOLO papers is given below:\n","- You Only Look Once: Unified, Real-Time Object Detection, 2015.<a href=\"https://pjreddie.com/media/files/papers/yolo_1.pdf\">YOLO Paper</a>\n","- YOLO9000: Better, Faster, Stronger, 2016.<a href=\"https://pjreddie.com/media/files/papers/YOLO9000.pdf\">YOLO9000 Paper</a>\n","- YOLOv3: An Incremental Improvement, 2018.<a href=\"https://pjreddie.com/media/files/papers/YOLOv3.pdf\">YOLOv3 Paper</a>\n"]},{"cell_type":"markdown","metadata":{"id":"ufKIk-HfeEeV"},"source":["### Questions\n","1. YOLO V3 Questions\n","a. What is the output grid size for this model of YOLO?\n","b. How many bounding boxes prototypes are allowed for each grid-based anchor box?\n","\n","2. NMS Threshold\n","a. Describe how the NMS algorithm works in your own words. Use an example from the images provided to illustrate your work.\n","b. What happens when the P(Objectness) (NMS Threshold) is set to zero?\n","NOTE: Don't try to run the system with NMS threshold of zero, it will be achingly slow and give many spurious detections.\n","\n","3. IoU Threshold\n","a. Describe how the IoU threshold works to remove overlapping boxes in an image?\n","b. Using the images provided, show how changing the threshold adds and removes boxes.\n","\n","4. Several images are provided in the image folder from the VOC data set. These images have some challenges such as object overlap and small objects. Run five images of your choice through the YOLO system with the same set of NMS and IoU thresholds and describe your results in terms of\n","a. Were the key objects found?\n","b. What was missed?\n","c. If there were multiples of the same object, were they all found correctly with appropriate bounding boxes?\n","d. Figure out how you can improve the system performance by changing the post-processing of the images."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10202,"status":"aborted","timestamp":1762888711730,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"},"user_tz":360},"id":"6giVY677XfSw"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive', force_remount=True)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","provenance":[]},"jupytext":{"formats":"ipynb,py:percent","main_language":"python"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}